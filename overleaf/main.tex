% WS proposal template for ICCV 2025
% V. Albiero, J. Tompkin

% Adopted from ECCV 2020, ECCV 2022, ICCV 2023 templates by M. Cho, B. Ham, A. Bartoli, A. Fusiello, A. Vedaldi, L. Karlinsky, T. Michaeli, K. Nishino


\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{xspace}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage[dvipsnames]{xcolor}
\usepackage{color,colortbl}
\usepackage{graphicx}
\usepackage{enumitem}
\newgeometry{vmargin={30mm, 30mm}, hmargin={30mm,30mm}} 
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\newcommand{\boldparagraph}[1]{\vspace{0.2em}\noindent\textbf{#1}}
\newcommand{\TODO}[1]{\textbf{\color{red}[TODO: #1]}}
\newcommand{\tentative}[1]{\textcolor{orange}{#1}}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cvprblue,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }



\newcommand\conf{CVPR 2026\xspace}
\newcommand{\URL}[1]{\texttt{#1}}
\title{\conf \\The 2nd Workshop on Multimodal Spatial Intelligence}
\author{}
\date{}


% instructions
\def\c#1{\textcolor{gray}{#1}}
% indicates parts to complete
\def\x{\textcolor{red}{xxx}}

\begin{document}

\maketitle

% The instructions are in \c{grey} and parts to complete are indicated by \x. Submit your proposal as a single pdf file named \texttt{ACRONYM.pdf}, where \texttt{ACRONYM} is the acronym of your proposed workshop.

\section{Summary}
\begin{tabular}{ll}
  \hline
  Workshop title & The 2nd Workshop on Multimodal Spatial Intelligence \\
  Acronym & MUSI\\
  Edition (1st, 2nd, ...) & 2nd \\
  Prior workshop series & \href{https://musi-workshop.github.io/}{1st MUSI Workshop} at ICCV 2025 \\
  Keywords & Spatial Reasoning, Multimodal Large Language Model, \\
   & World Models, Embodied AI, 3D Understanding\\
  Primary Subject Area & Foundation models (LLM, VLM, VLA, etc.) \\
  Secondary Subject Areas & Vision, language, and reasoning; \\
   & Multimodal learning; World models; \\
   & Embodied vision: Active agents, simulation; \\
   & Scene analysis and understanding \\
  Half or full day & Half \\
  Primary Organizer & Juil Koo (\href{mailto:63days@kaist.ac.kr}{\URL{63days@kaist.ac.kr}}) \\
  Secondary Organizer & Phillip Y. Lee (\href{mailto:phillip0701@kaist.ac.kr}{\URL{phillip0701@kaist.ac.kr}}) \\
  Anticipated audience size & 100-200 \\
  Paper submission & Yes (non-archival track only) \\
  Requested number of poster boards & 30 \\
  Special requests & None \\
  \hline
\end{tabular}

\section{Abstract}
Our multi-modal spatial intelligence (MUSI) workshop addresses how multimodal large language models (MLLMs) understand, reason about, and interact with spatial information from the physical world. The multimodal nature of spatial intelligence---requiring integration of images, videos, and 3D data---necessitates bringing together researchers from diverse domains: computer vision, robotics, graphics, and NLP. While recent MLLMs show promising visual-spatial capabilities, fundamental questions remain about spatial relationships, 3D environment modeling, and real-world spatial reasoning. This workshop explores how MLLMs learn spatial representations across modalities, advance world modeling and embodied AI, and address ethical considerations. We aim to establish benchmarks and foster cross-disciplinary collaboration to advance spatial reasoning in multimodal AI.

\section{Topics}
% \c{Topics that will be covered, audience, and relevance to the computer vision community and potential \conf attendees.}
% \x


Building on the success of \textbf{the 1st Workshop on \href{https://musi-workshop.github.io/}{Multimodal Spatial Intelligence (MUSI)} at ICCV 2025}, this 2nd edition continues to bring together researchers from computer vision, robotics, graphics, and NLP to advance the field at the intersection of visual understanding, multimodal learning, and embodied AI. We place multi-modal large language models (MLLMs) at the core of spatial intelligence, exploring how they can learn, interpret, and act on spatial information from images, videos, and 3D data.

We invite keynote talks on topics related to multi-modal spatial intelligence.
The workshop will cover topics including, but not limited to:
\begin{itemize}[noitemsep,leftmargin=*]
\item \textbf{Spatial Reasoning in Multimodal LLMs}: We consider multimodal LLMs (MLLMs) as the core of spatial intelligence. A key focus is to improve how MLLMs understand and reason about low-level to high-level spatial information using images, videos, and 3D representations.
\item \textbf{Beyond Conventional Perception with Multimodal Models}: Leveraging multimodal models to advance semantic and geometric reasoning in 2D/3D perception, reconstruction, and temporal scene understanding.
\item \textbf{Embodied AI and Spatial Intelligence for Agents}: Advancing MLLMs for real-world applications in assistive AI agents and autonomous systems, including planning, interaction, and navigation in complex environments.
% \item \textbf{Open-Vocabulary 2D/3D Perception}: Performing open-set 2D/3D object detection, segmenttaion, and reasoning across diverse environments
\item \textbf{World Models for Physical Understanding}: Interactively understanding, modeling, and reconstructing dynamic 3D scenes over time from multi-modal inputs.
\item \textbf{3D Understanding and Representation}: Advancing 3D scene understanding, generation, and manipulation through multimodal learning approaches.
\item \textbf{Trust, Ethics, and Societal Impact}: Addressing the reliability, biases, and ethical implications of spatial intelligence in AI systems.
\end{itemize}




\section{Organizers and Speakers}
\subsection{List of Organizers}
\begin{tabular}{llll}
Juil Koo & KAIST & \href{mailto:63days@kaist.ac.kr}{\URL{63days@kaist.ac.kr}} \\
Phillip Y. Lee & KAIST & \href{mailto:phillip0701@kaist.ac.kr}{\URL{phillip0701@kaist.ac.kr}} \\
Songyou Peng & Google DeepMind & \href{mailto:songyou@google.com}{\URL{songyou@google.com}}\\
Mikaela Angelina Uy & NVIDIA & \href{mailto:mikaelaangel@nvidia.com}{\URL{mikaelaangel@nvidia.com}}\\
Sanja Fidler & University of Toronto \& NVIDIA & \href{mailto:fidler@cs.toronto.edu}{\URL{fidler@cs.toronto.edu}}\\
Leonidas J. Guibas & Stanford University \& Google DeepMind & \href{mailto:guibas@cs.stanford.edu}{\URL{guibas@cs.stanford.edu}}\\
% \textcolor{orange}{Fei-Fei Li (TBD)} & \textcolor{orange}{Stanford University \& World Labs} & {\URL{feifeili@cs.stanford.edu}} \\
Minhyuk Sung & KAIST & \href{mailto:mhsung@kaist.ac.kr}{\URL{mhsung@kaist.ac.kr}} \\
\end{tabular}

\subsection{Organizers' Experience and Background}
% \c{Information that supports the organizers' ability to run a workshop, and appropriateness to a workshop on this topic, including brief bios as necessary.}
Several members of this organizing team successfully organized the \href{https://musi-workshop.github.io/}{1st Workshop on Multimodal Spatial Intelligence (MUSI)} at ICCV 2025, bringing together researchers from computer vision, robotics, graphics, and NLP. Building on this proven track record, the team is well-positioned to deliver an even more impactful 2nd edition at CVPR 2026.

The organizing team comprises diverse individuals from industry and various academic institutions with extensive experience in multi-modal large language models, spatial reasoning, and 3D scene understanding.

\paragraph{Juil Koo (KAIST)} \href{https://63days.github.io/}{\URL{63days.github.io}}\hfill \href{mailto:63days@kaist.ac.kr}{\URL{63days@kaist.ac.kr}}\\
Juil Koo is a Ph.D. student at KAIST, working on vision-language multi-modality and generative models. His early work \href{https://mhsung.github.io/publications/partglot}{PartGlot} was one of the pioneering studies in vision-language multi-modality and was awarded the Qualcomm Innovation Fellowship in 2022. 
%His research spans both understanding and generation across multi-modal domains.

\paragraph{Phillip Y. Lee (KAIST)} \href{https://phillipinseoul.github.io/}{\URL{phillipinseoul.github.io}}\hfill \href{mailto:phillip0701@kaist.ac.kr}{\URL{phillip0701@kaist.ac.kr}}\\
Phillip Y. Lee is a Ph.D. student at KAIST, working on multimodal large language models and spatial reasoning. 
His recent work \href{https://apc-vlm.github.io/}{Perspective-Aware Reasoning} have significantly influenced research on spatial reasoning in multimodal large language models, inspiring many follow-up works in the field.


\paragraph{Songyou Peng (Google DeepMind)} \href{https://pengsongyou.github.io/}{\URL{pengsongyou.github.io}}\hfill \href{mailto:songyou@google.com}{\URL{songyou@google.com}}\\
Songyou Peng is a research scientist at Google DeepMind, working on spatial intelligence in MLLM and world-scale 3D scene understanding. 
He was an Area Chair for ICCV'25, ICML'25 and 3DV'24. He is an experienced workshop organizer, having served as the primary organizer of the 1st \href{https://musi-workshop.github.io/}{MUSI workshop} at ICCV'25, and co-organized the \href{https://opensun3d.github.io}{OpenSUN3D workshop} at ICCV'23, CVPR'24, and ECCV'24, the \href{https://focus-workshop.github.io/}{FOCUS workshop} at ECCV'24, and the \href{https://scene-understanding.com/}{3D Scene Understanding workshop} at CVPR'25.

\paragraph{Mikaela Angelina Uy (NVIDIA)} \href{https://mikacuy.github.io/}{\URL{mikacuy.github.io/}}\hfill \href{mailto:mikaelaangel@nvidia.com}{\URL{mikaelaangel@nvidia.com}}\\
Mikaela Angelina Uy is a research scientist at NVIDIA Spatial Intelligence Lab, working on 3D geometry/shape analysis, scene understanding and spatial intelligence. She was selected as an EECS Rising Star and has also served as an area chair in 3DV '26 and program committee in Eurographics '25, '26.

\paragraph{Sanja Fidler (University of Toronto \& NVIDIA)} \hfill

\noindent\href{https://www.cs.utoronto.ca/~fidler/}{\URL{cs.utoronto.ca/~fidler}}\hfill \href{mailto:fidler@cs.toronto.edu}{\URL{fidler@cs.toronto.edu}}\\
Sanja Fidler is an associate professor at the University of Toronto and a director of AI at NVIDIA, where she leads the Spatial Intelligence Lab research lab in Toronto. Her research focuses on 3D vision, generative modeling, and embodied intelligence, bridging academic research and industrial AI. 
She has been a key organizer of several top-tier workshops and conferences on vision and AI, and her pioneering work on 3D generative models and world simulation has significantly advanced the field of spatial intelligence.

\paragraph{Leonidas J. Guibas (Stanford University \& Google DeepMind)} \hfill

\noindent\href{https://geometry.stanford.edu/?member=guibas}{\URL{geometry.stanford.edu/?member=guibas}}\hfill \href{mailto:guibas@cs.stanford.edu}{\URL{guibas@cs.stanford.edu}}\\
Leonidas J. Guibas is a full professor at Stanford University and a principal research scientist at Google DeepMind. A pioneer in geometric deep learning and 3D understanding, his work has fundamentally shaped spatial intelligence research. He recently received the prestigious Steven Anson Coons Award at SIGGRAPH'25. He has been a key organizer and chair of numerous major conferences in computer vision, machine learning, and 3D computer graphics, significantly influencing the development of spatial reasoning in AI.

\paragraph{Minhyuk Sung (KAIST)} \href{https://mhsung.github.io/}{\URL{mhsung.github.io}}\hfill \href{mailto:mhsung@kaist.ac.kr}{\URL{mhsung@kaist.ac.kr}}\\
Minhyuk Sung is an associate professor at KAIST working on spatial intelligence, with a focus on generating, manipulating, and analyzing visual data. His research bridges vision, language, and 3D geometry for multimodal spatial understanding. He is the recipient of the 2024 AsiaGraphics Young Researcher Award. He co-organized the Structural and Compositional Learning on 3D Data workshops at ICCV 2021 and CVPR 2023, and has served as an Area Chair or Technical Program Chair for major conferences including CVPR, ICCV, ECCV, SIGGRAPH Asia, NeurIPS, AAAI, and ICLR.



\subsection{List of Invited Speakers}
\paragraph{Ranjay Krishna (UW/AI2)} \href{https://www.ranjaykrishna.com}{\URL{ranjaykrishna.com}} \hfill \textbf{[\textcolor{Green}{Confirmed}]}\\
Ranjay Krishna is an assistant professor at the Paul G. Allen School of Computer Science \& Engineering. 
His research focuses on improving multimodal models’ spatial and temporal reasoning, particularly in the context of visual and linguistic understanding.
His recent works, including \emph{Molmo and PixMo}, \emph{Coarse Correspondences Boost Spatial-Temporal Reasoning in Multimodal Language Models}, and \emph{Perception Tokens Enhance Visual Reasoning in Multimodal Language Models}, are highly relevant to the workshop.
His expertise directly contributes to discussions on leveraging multimodal large language models (MLLMs) to enhance 2D and 3D spatial intelligence, making him a key speaker for our event.

\paragraph{Saining Xie (NYU/Google DeepMind)}
\href{https://www.sainingxie.com/}{\URL{sainingxie.com}} \hfill \textbf{[\textcolor{Green}{Confirmed}]}\\
Saining Xie is an assistant professor at New York University and a research scientist at Google DeepMind.
His research focuses on advancing robust visual intelligence by building scalable and reliable systems that can interpret visual events, answer questions on demand, and develop a common-sense understanding of the world.
His recent works, including \emph{Cambrian-1}, \emph{V-IRL}, \emph{Thinking in Space} and \emph{Eyes Wide Shut} are highly relevant to our workshop.
His expertise in vision-centric multimodal LLMs and their spatial reasoning capabilities makes him a key contributor to discussions on improving 2D and 3D spatial intelligence.

\paragraph{Katerina Fragkiadaki (CMU)}
\href{https://www.cs.cmu.edu/~katef/}{\URL{cs.cmu.edu/~katef}}\hfill \textbf{[\textcolor{Green}{Confirmed}]}\\
Katerina Fragkiadaki is an associate professor at Carnegie Mellon University. Her research focuses on learning visual predictive models of the world, physical reasoning, and embodied AI. Her work on video understanding and anticipation, as well as learning world models for robotic manipulation, is highly relevant to spatial intelligence in multimodal systems.

\paragraph{Chuang Gan (UMass Amherst / MIT-IBM Watson AI Lab)} \hfill \textbf{[\textcolor{Green}{Confirmed}]}\\
\href{https://embodied-agi.cs.umass.edu/}{\URL{embodied-agi.cs.umass.edu}} \\
Chuang Gan is an assistant professor at UMass Amherst and a principal research scientist at the MIT-IBM Watson AI Lab. His research focuses on visual reasoning, video understanding, and embodied AI. His work on commonsense reasoning and physical understanding in vision systems is highly relevant to spatial intelligence and world modeling.

\paragraph{Angel X. Chang (SFU)} 
\href{https://angelxuanchang.github.io/}{\URL{angelxuanchang.github.io}} \hfill \textbf{[\textcolor{Green}{Confirmed}]}\\
Angel X. Chang is an associate professor at Simon Fraser University. Her research focuses on 3D scene understanding, language grounding in 3D environments, and embodied AI. She is known for her work on 3D datasets and benchmarks, making her expertise highly relevant to bridging language and 3D spatial understanding.

\paragraph{Roozbeh Mottaghi (FAIR / UW)} 
\href{https://roozbehm.info/}{\URL{roozbehm.info}} \hfill \textbf{[\textcolor{Green}{Confirmed}]}\\
Roozbeh Mottaghi is a research scientist at Meta FAIR and an affiliate assistant professor at the University of Washington. His research focuses on embodied AI, visual navigation, and 3D scene understanding. His work on learning spatial representations for embodied agents is highly relevant to the workshop's focus on spatial intelligence.

\paragraph{Kristen Grauman (UT Austin)}
\href{https://www.cs.utexas.edu/~grauman/}{\URL{cs.utexas.edu/~grauman}}\hfill \textbf{[\textcolor{Green}{Confirmed}]}\\
Kristen Grauman is a full professor at the University of Texas at Austin. Her research focuses on multimodal perception, embodied AI, and video understanding, with emphasis on vision for robotics and perception for action. Her work on first-person egocentric computer vision, navigation and exploration in 3D spaces, and audio-visual learning from video is highly relevant to spatial intelligence and multimodal understanding in physical environments.


\subsection{Diversity}
% \c{Discuss how diversity is being addressed among (a) the organizing committee and (b) the invited speakers.}
The proposed goals of the workshop will be accomplished by inviting leaders from relevant Computer Vision, Multi-Modal LLM, and Robotics fields.
We will have a line-up of seven speakers representing diverse academic institutions (UW, NYU, CMU, UMass Amherst, SFU, UT Austin) and industry research labs (AI2, Google DeepMind, MIT-IBM Watson AI Lab, Meta FAIR) with diverse backgrounds and seniority levels. 
The organizing team consists of 7 members, with a balanced mix of academic and industry research scientists representing various cultural backgrounds and career stages, from Ph.D. students to full professors. \textbf{We maintain strong gender diversity across both our invited speakers (3 female, 4 male) and organizing team (2 female, 5 male).} The team comprises individuals with diverse expertise spanning multimodal learning, spatial reasoning, 3D understanding, and embodied AI, providing a rich blend of perspectives and experiences. With the objective of bringing together different subfields, the workshop is anticipated to attract participants from varying backgrounds and areas of expertise, encouraging lively cross-disciplinary discussions and exchanges.


\section{Format and Logistics}
% \conf will be in-person. There may be remote access provided by the conference (e.g., Zoom) but this is to be decided; workshop organizers may also arrange their own.
% The main goal of our workshop is to build and foster the community around the rising topic of spatial intelligence in multi-modal large language models.
Building on the successful format of the 1st MUSI workshop, we plan for a packed \textbf{half-day} in-person workshop.
We anticipate 100--200 in-person participants.

\boldparagraph{Half-Day Workshop Format.}
To accommodate our lineup of 7 confirmed speakers within a half-day schedule, we have designed a focused program:
\begin{itemize}[noitemsep,leftmargin=*]
\item \textbf{Keynote Talks}: Invited talks will be 30 minutes each (25 min talk + 5 min Q\&A), allowing for in-depth coverage of topics.
\item \textbf{Integrated Poster Session}: A vibrant poster session during the extended coffee break to maximize community interaction.
\end{itemize}

We intend to stream the entire workshop, including keynote talks and Q\&A sessions, with virtual participants able to engage through the chat.

We will organize an in-person poster session to provide a valuable platform for engaging the audience and actively connecting with the community. All accepted works will be presented as posters and uploaded to the website for virtual attendees.


\subsection{Schedule}
\definecolor{orga}{RGB}{214, 234, 248}
\definecolor{keyn}{RGB}{252, 243, 207}
\definecolor{chal}{RGB}{213, 245, 227}
\definecolor{disc}{RGB}{250, 219, 216}
\definecolor{poster}{RGB}{230, 230, 250}

\begin{tabular}{lll}
  \toprule
  \rowcolor{poster} 08:00 -- 08:30 & Poster Setup and Early Morning Poster Session & 30 min\\
  \rowcolor{orga} 08:30 -- 08:40 & Welcome \& Introduction              & 10 min\\
  \rowcolor{keyn} 08:40 -- 09:10 & Keynote Talk 1 (25 min + 5 min Q\&A) & 30 min\\
  \rowcolor{keyn} 09:10 -- 09:40 & Keynote Talk 2 (25 min + 5 min Q\&A) & 30 min\\
  \rowcolor{keyn} 09:40 -- 10:10 & Keynote Talk 3 (25 min + 5 min Q\&A) & 30 min\\
  \rowcolor{disc} 10:10 -- 10:25 & Coffee Break                         & 15 min\\
  \rowcolor{keyn} 10:25 -- 10:55 & Keynote Talk 4 (25 min + 5 min Q\&A) & 30 min\\
  \rowcolor{keyn} 10:55 -- 11:25 & Keynote Talk 5 (25 min + 5 min Q\&A) & 30 min\\
  \rowcolor{keyn} 11:25 -- 11:55 & Keynote Talk 6 (25 min + 5 min Q\&A) & 30 min\\
  \rowcolor{keyn} 11:55 -- 12:25 & Keynote Talk 7 (25 min + 5 min Q\&A) & 30 min\\
  \rowcolor{orga} 12:25 -- 12:55 & Closing Poster Session and Remarks   & 30 min\\
\bottomrule
\end{tabular}

\vspace{0.5em}
\noindent\textbf{Note:} We plan to have approximately 30 posters displayed during the coffee break.



%===================== Call for Papers =====================%
\subsection{Paper Submission}
% \tentative{\textbf{[Tentative - Pending Final Confirmation]}}

We are planning to include a call for papers to encourage broader community participation and showcase cutting-edge research in multimodal spatial intelligence. The submission format and review process are outlined below:
\newline


\boldparagraph{Submission Guidelines:}

We welcome both new work and papers previously accepted at other venues. For new work, papers must be submitted in the CVPR 2026 format, with a length of 2-8 pages (excluding references). Previously accepted papers may be submitted in their original format.

Submissions should be made through the OpenReview submission system.

\boldparagraph{Topics:}
We invite submissions on topics including, but not limited to:
\begin{itemize}[noitemsep,leftmargin=*]
    \item Spatial Reasoning in Multimodal LLMs
    \item World Models for Physical Understanding
    \item Embodied Agents and Vision-Language-Action (VLA) Models
    \item 3D Scene Understanding, Generation, and Reconstruction
    \item Open-Vocabulary 2D/3D Perception and Reasoning
    \item Temporal and Causal Reasoning in Dynamic Environments
    \item Multimodal Interaction, Grounding, and Planning
    \item Neuro-symbolic Approaches for Spatial Intelligence
    \item Benchmarks and Datasets for Spatial Reasoning
    \item Trust, Ethics, and Societal Impact of Spatial AI
\end{itemize}

\boldparagraph{Review Process:}

\begin{itemize}[noitemsep,leftmargin=*]
\item Double-blind peer review process through the OpenReview system.
\item Program Committee: We will invite 20-30 reviewers from leading research institutions and industry labs with expertise in multimodal learning, spatial reasoning, 3D vision, and embodied AI. The program committee includes Juil Koo (KAIST), Phillip Y. Lee (KAIST), Mikaela Angelina Uy (NVIDIA), Minhyuk Sung (KAIST), and other experts in the field.
\item Each paper will receive at least 2-3 reviews.
\end{itemize}

\boldparagraph{Timeline:}

\begin{itemize}[noitemsep,leftmargin=*]
\item Paper submission deadline: March 13, 2026 
\item Notification to authors: April 3, 2026
\item Camera-ready deadline: April 17, 2026
\end{itemize}

\boldparagraph{Publication:}
The workshop will be non-archival. Authors of accepted papers retain the full copyright of their work and are free to submit extended versions to conferences or journals.
%============================================================%

% \x
% \subsection{Competition}
% \c{If the workshop hosts a competition, describe it: (a) Explain which datasets will be used, (b) Whether the datasets are already available or not; in the latter case, provide an estimate when the datasets will be available and describe your contingency plan in case of delays; (c) Ethical considerations for the datasets, (d) How submissions will be evaluated, (e) The timeline for the competition (start, submission deadline, decisions to participants).}
% \x
% \subsection{Special Requests}
% % \c{State special space or equipment requests, if any.}
% % \x
% None

\section{Broader Impacts}
The ability to robustly perceive, reason about, and interact with the 3D world is a major milestone toward more capable and reliable AI systems. Recent advancements in multi-modal large language models (MLLMs) have demonstrated remarkable visual-spatial intelligence, enabling sophisticated scene understanding and decision-making that extends far beyond traditional perception tasks. These developments hold transformative potential across diverse applications including robotics, autonomous systems, AR/VR, assistive technologies, and healthcare.

Building on the success of the 1st MUSI workshop at ICCV 2025, this 2nd edition aims to further advance the field by bringing together an expanded and diverse community of researchers from academia and industry. Our call for papers will encourage broader participation and facilitate the exchange of cutting-edge research ideas, fostering collaboration and accelerating progress in spatial intelligence.

However, the rapid advancement of MLLMs also introduces significant societal concerns, including data biases, fairness issues, privacy risks, real-world reliability, potential for misuse, and environmental impact. To address these critical challenges responsibly, we will incorporate ethical considerations throughout keynote talks and Q\&A sessions, openly examining potential risks, societal implications, and strategies for responsible development and deployment.

Through this workshop, our goal is to:
\begin{itemize}[noitemsep,leftmargin=*]
\item Foster interdisciplinary collaboration among researchers across computer vision, NLP, robotics, and AI ethics
\item Define and refine evaluation metrics, tasks, and benchmarks for spatial intelligence
\item Promote transparency, interpretability, and accountability in multimodal spatial reasoning systems
\item Encourage discussions on responsible AI practices and societal impact
\item Build a sustainable and inclusive research community dedicated to advancing spatial intelligence
\end{itemize}



% \subsection{Social considerations}
% \c{Tell us social considerations around the topic (if any)}
% \x
% \subsection{Ethical considerations}
% \c{Tell us ethical considerations around the topic (if any)}
% \x

% Multi-modal LLMs that exhibit visual-spatial intelligence can significantly benefit areas such as robotics, AR/VR, and assistive technologies, yet also pose risks related to biased data, surveillance, and misuse. 

\section{Relationship to Previous Workshops}
% \c{Describe how this proposal relates to previous workshops held at CVPR/ICCV/ECCV/etc. in the last three years.}
% \x

\subsection{Building on the 1st MUSI Workshop}
This proposal builds directly on the success of the \href{https://musi-workshop.github.io/}{1st Workshop on Multimodal Spatial Intelligence (MUSI)} held at ICCV 2025. The inaugural workshop successfully brought together researchers from computer vision, robotics, graphics, and NLP to explore how multi-modal large language models can learn, interpret, and act on spatial information. 

For this 2nd edition at CVPR 2026, we aim to expand the scope and impact by:
\begin{itemize}[noitemsep,leftmargin=*]
\item Hosting a packed half-day session to facilitate focused, high-energy discussions.
\item Expanding from 5 to 7 invited speakers with broader expertise across spatial intelligence, embodied AI, and world modeling.
\item Introducing a call for papers to encourage broader community participation and showcase cutting-edge research.
\item Incorporating ethical considerations and responsible AI discussions throughout keynote talks and Q\&A sessions.
\end{itemize}

\subsection{Differences to Other Related Workshops}

Several workshops at recent conferences have explored related themes. We categorize them and highlight our unique positioning:

\begin{itemize}[leftmargin=*]
    \item \textbf{Embodied AI and Robotics Workshops.}
    \begin{itemize}[noitemsep,leftmargin=*]
        \item \textit{Embodied Spatial Reasoning} at ICCV'25
        \item \textit{Embodied AI Workshop} at CVPR'25
        \item \textit{Bridging Language, Vision and Action in 3D Environments} at CVPR'25
    \end{itemize}
    These workshops focus on embodied agents, robotic applications, and vision-language-action models. While we share interest in embodied systems, \textbf{our workshop places MLLMs and their spatial intelligence at the center}, examining spatial understanding as a cognitive capability that extends beyond robotic control and navigation to encompass world modeling, temporal reasoning, and physical scene comprehension.

    \item \textbf{3D Scene Understanding Workshops.}
    \begin{itemize}[noitemsep,leftmargin=*]
        \item \textit{Open-World 3D Scene Understanding (OpenSUN3D)} series at ICCV'23, ECCV'24, CVPR'24, '25
        \item \textit{3D Scene Understanding for Vision, Graphics, and Robotics} series at CVPR'21, '23, '25
    \end{itemize}
    These workshops focus on open-set 3D perception and vision-centric 3D representations. Our workshop \textbf{builds on these foundations but goes further by investigating how MLLMs develop deeper spatial reasoning capabilities}—moving beyond detection and segmentation to encompass spatial relationships, physical properties, and multimodal scene comprehension.

    \item \textbf{Multimodal Reasoning and World Models.}
    \begin{itemize}[noitemsep,leftmargin=*]
        \item \textit{Multi-Modal Reasoning for Agentic Intelligence (MMRAgI)} at ICCV'25
        \item \textit{Reliable and Interactable World Models (RIWM)} at ICCV'25
    \end{itemize}
    These workshops address broad multimodal reasoning and emphasize physical simulation and interactivity. \textbf{Our workshop specifically targets spatial intelligence in MLLMs}, examining how these models learn and represent spatial knowledge through language and vision, bridging the cognitive and linguistic aspects of spatial understanding with world modeling and physical reasoning.
\end{itemize}

\subsection{What Makes Our Workshop Unique}
\begin{itemize}[leftmargin=*]
    \item \textbf{MLLMs at the Core of Spatial Intelligence.}
    We place multi-modal LLMs at the core, examining how they learn, interpret, and act on spatial information from images, videos, and 3D data, enabling deeper scene comprehension in the physical world.
    \item \textbf{Beyond Conventional Perception.}
    Unlike previous workshops that focus on perception tasks like detection and segmentation, we aim to explore how MLLMs develop deeper spatial reasoning, including 3D world modeling, planning, interaction, and memory.
    With contributions from experts in spatial reasoning in multimodal LLMs (Ranjay Krishna, Saining Xie), embodied AI and navigation (Kristen Grauman, Roozbeh Mottaghi), world models and physical reasoning (Katerina Fragkiadaki, Chuang Gan), and 3D scene understanding (Angel X. Chang), we will discuss how new models move beyond static understanding to dynamic real-world environments.
    \item \textbf{Multi-Disciplinary Scope.}
    Spatial intelligence spans multiple domains. Our workshop brings together top researchers from computer vision, robotics, graphics, and NLP to tackle challenges at the intersection of visual understanding, multimodal learning, and embodied AI.
    \item \textbf{Proven Track Record and Growing Community.}
    Building on the success of the 1st MUSI workshop at ICCV 2025, this 2nd edition demonstrates sustained momentum in the field. With our call for papers, we aim to further expand the community and foster collaborations that tackle complex challenges in spatial intelligence research.
    \item \textbf{Forward-Looking Goals and Benchmarking.}
    We encourage new datasets and benchmarks that target spatial reasoning (e.g., \href{https://vision-x-nyu.github.io/thinking-in-space.github.io/}{Thinking in Space}), pushing beyond traditional open-vocabulary 3D tasks to evaluate deeper spatial understanding, world modeling, and physical reasoning capabilities.
\end{itemize}

\newpage

% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{bibliography}
% }

\end{document}
ㄹ